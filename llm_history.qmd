---
title: "From n-grams to ChatGPT: A Very Brief History of Language Modeling"
format: 
    revealjs:
        incremental: true
        scrollable: true
        smaller: true
        controls: true
        controls-layout: "edges"
        slide-number: false
        logo: "assets/graph_courses_logo.png"
        overview: true
        chalkboard: true
        progress: true
        code-copy: true
        css: custom.css
        transition: "slide"
        background-transition: "slide"
        auto-animate: true
        auto-animate-easing: "ease"
        auto-animate-duration: 0.8
author: "Your Name/Affiliation"
---

From n-grams to ChatGPT: A Very Brief History of Language Modeling

## Why the Past Matters 

*   Technical capabilities are compounding → forecasting requires a historical lens
*   Pitfalls (bias, hallucination, cost) trace straight back to each training step
*   Goal today: give life-science graduates a mental model, not just a recipe

::: notes
Note that GPT-1 is only *seven* years old; the pace is **accelerating**. 
 
Forecasting the next two years is challenging without this backstory.
:::

---

## What is a language model?

.... definition...

## One-page lineage table 

*A summary of the key steps in model evolution we'll discuss.*

| Model (year)                     | Incremental training steps                                       | Corpus size (tokens ≈ "Bibles"*)       | Max context (tokens) | Rough train compute           | Typical answer to "Who is the current pope? (3 May 2025)"                                | Typical inference HW |
| -------------------------------- | ---------------------------------------------------------------- | -------------------------------------- | -------------------- | ----------------------------- | ---------------------------------------------------------------------------------------- | ---------------------- |
| **4-gram (Brown, 1960s)**      | Count 4-grams, MLE                                               | 1 M ≈ 1.3 Bibles                       | 3 (last three words) | < 1 CPU-h                     | "the pope is the ..." (nonsense)                                                         | any laptop CPU         |
| **GPT-1 → GPT-3 (2018-20)**    | Same next-token objective, **massive scale-up** of params & data | 0.8 B → 500 B ≈ 1 K → 640 K Bibles ↑ | 512 → 2 048 ↑        | ≈3 × 10²³ FLOP ≈ 355 V100-years ↑ | "Read our new article on the pope here." (fluent but ignores the question's intent)        | single GPU → multi-GPU |
| **GPT-3.5 / ChatGPT (2022)**   | + Supervised instruction fine-tune + RLHF                        | ≈ 500 B + few × 10⁵ labelled prompts | 4 096 ↑              | + a few V100-days ↑           | "Pope Francis." (Model follows instruction but is **stale**; unaware of April 2025 death.) | cloud GPU              |
| **GPT-4o / GPT-4 Turbo (23-24)** | + Tool-use fine-tune (plugins: Browse, Code Interpreter)       | ≈ 1 T ≈ 1.6 M Bibles ↑                 | 8 K / 128 K ↑        | ≈ 2.5 M A100-days ↑           | "Pope Francis died on 14 Apr 2025; Pope John XXIV was elected on 28 Apr 2025." (cites news) | cloud GPU + API calls  |
| **GPT-4o1 (reasoning, 24-25)**   | + Chain-of-thought reinforcement (post-training)                 | same ≈ 1 T                             | 8 K – 128 K          | + ≈ 10–20 k A100-days ↑       | Gives same facts **and** step-by-step justification (citing sources).                    | cloud GPU + tools      |

*Bible rough equivalence uses ≈ 780,000 words per King James Bible. Pope example is hypothetical.*

::: notes
Introduce this as the roadmap. We'll walk through each row, explaining the *incremental* changes and their consequences, then return here for a summary.
:::

---

## Baseline: N-gram Language Models 

*   **Core Idea:** Predict the next word based *only* on the previous N-1 words.
*   **Training:** Simply count sequences of N words in a text corpus (Maximum Likelihood Estimation - MLE).
*   **Limitation:** Understands local word patterns but has no broader context, world knowledge, or concept of instructions/questions.

| Model (year)               | Incremental training steps | Corpus size (tokens ≈ "Bibles"*) | Max context (tokens) | Rough train compute | Typical answer to "Who is the current pope? (3 May 2025)" | Typical inference HW |
| -------------------------- | -------------------------- | -------------------------------- | -------------------- | ------------------- | ---------------------------------------------------------- | -------------------- |
| **4-gram (Brown, 1960s)** | Count 4-grams, MLE       | 1 M ≈ 1.3 Bibles                 | 3 (last three words) | < 1 CPU-h           | "the pope is the ..." (nonsense)                           | any laptop CPU       |

---
***References for this slide:***
*   **Brown Corpus:** [Wikipedia](https://en.wikipedia.org/wiki/Brown_Corpus)

::: notes

**(Note: We'll switch to a live demo here to show trigram/4-gram output.)**
:::

## N-grams super small but kinda useful

google search prediction (image of that ) etc.

---

## Need a smarter way

Enter the transformer. 

*   **Key Innovation:** The Transformer Architecture (Vaswani et al., 2017) is a much more intelligent... and much more parallelizable... way to do next token prediction.
*   **Approach:** Keep the same core objective (predict the next token) but...

[IMAGE showing how it works]

next token prediction. 

What does GPT stand for

## The training data

Cuz transformer is so good at this, we can train it on ALL the data you can get your hands on. Basically anything on the internet...

See NYT lawsuit.

## GPT-1 → GPT-3: Scaling the Transformer 

::: notes
**(Note: Live demo of a GPT-2/3 base model answer: *"The current pope is discussed in several recent theological journals. One article explores..."* – shows fluency but avoidance.)**
:::

| Model (year)                | Incremental training steps                                       | Corpus size (tokens ≈ "Bibles"*)       | Max context (tokens) | Rough train compute           | Typical answer to "Who is the current pope? (3 May 2025)"             | Typical inference HW |
| --------------------------- | ---------------------------------------------------------------- | -------------------------------------- | -------------------- | ----------------------------- | --------------------------------------------------------------------- | ---------------------- |
| **GPT-1 → GPT-3 (2018-20)** | Same next-token objective, **massive scale-up** of params & data | 0.8 B → 500 B ≈ 1 K → 640 K Bibles ↑ | 512 → 2 048 ↑        | ≈3 × 10²³ FLOP ≈ 355 V100-years ↑ | "Read our new article on the pope here." (fluent but ignores intent) | single GPU → multi-GPU |

---
***References for this slide:***
*   **Transformer:** Vaswani, A., et al. (2017). Attention is All You Need. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
*   **GPT-3 Scale/Compute:** [Lambda Labs Blog (GPT-3 Demystified)](https://lambdalabs.com/blog/demystifying-gpt-3)
---

## But can the model chat?

if we train on chat Q&As...

## GPT-3.5 / ChatGPT: Aligning with User Intent

::: notes

**(Note: Live demo of GPT-3.5 answer, highlighting helpfulness but staleness.)**
:::


*   **Key Innovation:** Post-training "Alignment" to make the base model more helpful and follow instructions.
    *   **Supervised Fine-Tuning (SFT):** Train on examples of desired input/output pairs (e.g., Q&A format).
    *   **Reinforcement Learning from Human Feedback (RLHF):** Use human preferences to rank model outputs, training a reward model to guide the LLM towards helpful, harmless, and honest responses.
*   **Result:** Model now understands it should *answer* questions, be conversational, and adhere to safety guidelines. 

One important limitation: But its knowledge is frozen at its pre-training data cutoff.


| Model (year)                 | Incremental training steps                   | Corpus size (tokens ≈ "Bibles"*)       | Max context (tokens) | Rough train compute | Typical answer to "Who is the current pope? (3 May 2025)"                           | Typical inference HW |
| ---------------------------- | -------------------------------------------- | -------------------------------------- | -------------------- | ------------------- | --------------------------------------------------------------------------------- | -------------------- |
| **GPT-3.5 / ChatGPT (2022)** | + Supervised instruction fine-tune + RLHF | ≈ 500 B + few × 10⁵ labelled prompts | 4 096 ↑              | + a few V100-days ↑ | "Pope Francis." (Model follows instruction but is **stale**; unaware of Apr 2025 death) | cloud GPU            |

---
***References for this slide:***
*   **Instruction-tuning / RLHF:** Ouyang, L., et al. (2022). Training language models to follow instructions with human feedback. [arXiv:2203.02155](https://arxiv.org/abs/2203.02155)


---

## GPT-4o / GPT-4 Turbo: Augmenting with Tools 

::: notes

**(Note: Live demo of GPT-4o with browsing for the pope question.)**
:::

- We saw that training was fixed. And also didn't really do much. 

*   **Key Innovation:** Fine-tuning the model to use external "tools" via function calling / plugins.
    *   Model learns to recognize when a task requires external info (web search) or computation (running code).
    *   It generates a structured request (e.g., `search("current pope")`), receives the tool's output, and synthesizes the final answer.

*   **Result:** Overcomes stale knowledge and improves capabilities like calculation or accessing real-time data. Introduces new failure modes (choosing wrong tool, bad data from tool).

| Model (year)                     | Incremental training steps                                 | Corpus size (tokens ≈ "Bibles"*) | Max context (tokens) | Rough train compute | Typical answer to "Who is the current pope? (3 May 2025)"             | Typical inference HW    |
| -------------------------------- | ---------------------------------------------------------- | -------------------------------- | -------------------- | ------------------- | -------------------------------------------------------------------- | ----------------------- |
| **GPT-4o / GPT-4 Turbo (23-24)** | + Tool-use fine-tune (plugins: Browse, Code Interpreter) | ≈ 1 T ≈ 1.6 M Bibles ↑           | 8 K / 128 K ↑        | ≈ 2.5 M A100-days ↑ | "Pope Francis died ... Pope John XXIV was elected..." (cites news) | cloud GPU + API calls |

---
***References for this slide:***
*   **Tool Use:** [OpenAI Blog (Plugins)](https://openai.com/index/chatgpt-plugins/), [OpenAI Blog (DevDay - Functions/Tools)](https://openai.com/index/new-models-and-developer-products-announced-at-devday/)
*   **GPT-4 Compute Estimates:** Patel, A. B., et al. (2025). SpecInF... [arXiv:2503.02550v3](https://arxiv.org/html/2503.02550v3) (Note: Estimates vary)
*   **Hypothetical News (Example):** [Reuters (Example)](https://www.reuters.com/world/pope-francis-has-died-vatican-says-video-statement-2025-04-21/), [Vatican News (Example)](https://www.vaticannews.va/en/vatican-city/news/2025-04/conclave-elect-new-pope-cardinals-beginning-date-may-2025.html)


## GPT-4o1: Optimizing for Reasoning 

::: notes
Analogy: Forcing students to "show their work". Reduces likelihood of confident guessing on complex problems. Not foolproof - the reasoning chain can be flawed but internally consistent.

**(Note: Live demo of GPT-4o1 showing reasoning steps for the pope question, assuming available.)**
:::


*   **Key Innovation:** Explicitly training the model (post-training) to perform and verify step-by-step reasoning ("Chain of Thought" - CoT).
    *   Uses RL to reward models that generate a logical intermediate thought process before giving the final answer.
    *   Focuses on improving performance on complex tasks requiring multi-step logic (math, coding, science).
*   **Result:** Significantly better benchmark scores on reasoning tasks. Reduced hallucination *on the reasoning path itself*, though errors can still occur. Slower inference due to generating intermediate steps.

| Model (year)                   | Incremental training steps                       | Corpus size (tokens ≈ "Bibles"*) | Max context (tokens) | Rough train compute     | Typical answer to "Who is the current pope? (3 May 2025)"                    | Typical inference HW |
| ------------------------------ | ------------------------------------------------ | -------------------------------- | -------------------- | ----------------------- | ---------------------------------------------------------------------------- | -------------------- |
| **GPT-4o1 (reasoning, 24-25)** | + Chain-of-thought reinforcement (post-training) | same ≈ 1 T                       | 8 K – 128 K          | + ≈ 10–20 k A100-days ↑ | Gives same facts **and** step-by-step justification (citing sources).        | cloud GPU + tools    |

---
***References for this slide:***
*   **GPT-4o1 Reasoning:** [OpenAI Blog (o1-preview)](https://openai.com/index/introducing-openai-o1-preview/)
*   **(Optional) Chain of Thought:** Wei, J., et al. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. [arXiv:2201.11903](https://arxiv.org/abs/2201.11903) (Concept)

---
## analogy for GPT 3 -> 3.5 -> 4o -> 4o series.

Gpt 3. Read the textbook

3.5 Read a lot more textbooks + focus on the examples with Solutions so you know how to answer questions. 

4 Read a lot more! + Learn to use a calculator, the internet etc. to improve how you answer questions. 

o1: Read the practice questions, try to solve them, and update how you think based on which reasoning paths arrive at the right answer.

## One-page lineage table (Summary) 

*Let's revisit the full progression.*

| Model (year)                     | Incremental training steps                                       | Corpus size (tokens ≈ "Bibles"*)       | Max context (tokens) | Rough train compute           | Typical answer to "Who is the current pope? (3 May 2025)"                                | Typical inference HW |
| -------------------------------- | ---------------------------------------------------------------- | -------------------------------------- | -------------------- | ----------------------------- | ---------------------------------------------------------------------------------------- | ---------------------- |
| **4-gram (Brown, 1960s)**      | Count 4-grams, MLE                                               | 1 M ≈ 1.3 Bibles                       | 3 (last three words) | < 1 CPU-h                     | "the pope is the ..." (nonsense)                                                         | any laptop CPU         |
| **GPT-1 → GPT-3 (2018-20)**    | Same next-token objective, **massive scale-up** of params & data | 0.8 B → 500 B ≈ 1 K → 640 K Bibles ↑ | 512 → 2 048 ↑        | ≈3 × 10²³ FLOP ≈ 355 V100-years ↑ | "Read our new article on the pope here." (fluent but ignores the question's intent)        | single GPU → multi-GPU |
| **GPT-3.5 / ChatGPT (2022)**   | + Supervised instruction fine-tune + RLHF                        | ≈ 500 B + few × 10⁵ labelled prompts | 4 096 ↑              | + a few V100-days ↑           | "Pope Francis." (Model follows instruction but is **stale**; unaware of April 2025 death.) | cloud GPU              |
| **GPT-4o / GPT-4 Turbo (23-24)** | + Tool-use fine-tune (plugins: Browse, Code Interpreter)       | ≈ 1 T ≈ 1.6 M Bibles ↑                 | 8 K / 128 K ↑        | ≈ 2.5 M A100-days ↑           | "Pope Francis died on 14 Apr 2025; Pope John XXIV was elected on 28 Apr 2025." (cites news) | cloud GPU + API calls  |
| **GPT-4o1 (reasoning, 24-25)**   | + Chain-of-thought reinforcement (post-training)                 | same ≈ 1 T                             | 8 K – 128 K          | + ≈ 10–20 k A100-days ↑       | Gives same facts **and** step-by-step justification (citing sources).                    | cloud GPU + tools      |

*Bible rough equivalence uses ≈ 780,000 words per King James Bible. Pope example is hypothetical.*


## Where are things going? 

Based on this history, future trends likely include:

*   **Continued Rapid Evolution:** The pace from GPT-1 (2018) to GPT-4o1 (2024) suggests increasing capability... significant, perhaps unpredictable, changes ahead (true multi-modality, agentic systems, new architectures?).
*   **Context Windows Expanding:** Following the trend... Expect models handling larger documents/conversations natively.

Some limitations will likely persist partly:

*   **Bias**: Trained on the internet. Reflects whatever it is trained o.
**Stale Knowledge**
    *   *Cause:* Models only know what's in their training data, which has a cutoff date. So you need to be sure they are searchin if you wanna
*   **Compute Cost / Energy**: - Cost: A direct consequence of the "bigger is better" scaling paradigm. Very hard to run good models on your computer. So the privacy issues will continue to be a thing. 
*   **Context Window Limits**: But the idea that models have context windows continuess. Before we can get agents that are human-like....how much context does a typical human see in a year? as an example...??

